\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[11pt,ignorenonframetext,]{beamer}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
\usetheme[]{metropolis}
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\usepackage{hyperref}
\hypersetup{
            pdftitle={Lecture 10},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\newif\ifbibliography
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\newenvironment{Shaded}{}{}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
% Prevent slide breaks in the middle of a paragraph:
\widowpenalties 1 10000
\raggedbottom
\setbeamertemplate{part page}{
\centering
\begin{beamercolorbox}[sep=16pt,center]{part title}
  \usebeamerfont{part title}\insertpart\par
\end{beamercolorbox}
}
\setbeamertemplate{section page}{
\centering
\begin{beamercolorbox}[sep=12pt,center]{part title}
  \usebeamerfont{section title}\insertsection\par
\end{beamercolorbox}
}
\setbeamertemplate{subsection page}{
\centering
\begin{beamercolorbox}[sep=8pt,center]{part title}
  \usebeamerfont{subsection title}\insertsubsection\par
\end{beamercolorbox}
}
\AtBeginPart{
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \frame{\subsectionpage}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{geometry}
\usepackage{graphicx}

\usepackage{bbold}
\usepackage{lmodern}


\usepackage{url}		% produces hyperlinks

\usepackage{colortbl}	% allows for color usage in tables
\usepackage{multirow}	% allows for rows that span multiple rows in tables

\usepackage{color}          	% gives color options
\usepackage{xcolor}		% this package has a variety of color options

\usepackage{multicol}
\usepackage{textcomp}

\usepackage{setspace}
\usepackage{changepage}
\usepackage{isotope}

\singlespacing

%%%%%%%%%%%%%%%%
% Small code output
%%%%%%%%%%%%%%%%

%% change fontsize of R code

\makeatletter
\@ifundefined{Shaded}{\newenvironment{Shaded}{}{}}{}
\makeatother


\let\oldShaded\Shaded
\let\endoldShaded\endShaded
\renewenvironment{Shaded}{\footnotesize\begin{spacing}{0.9}\oldShaded}{\endoldShaded\end{spacing}}

%% change fontsize of output
\let\oldverbatim\verbatim
\let\endoldverbatim\endverbatim
\renewenvironment{verbatim}{\footnotesize\begin{spacing}{0.9}\oldverbatim}{\endoldverbatim\end{spacing}}


\newcommand{\tinyoutput}{
  \renewenvironment{Shaded}{\tiny\begin{spacing}{0.9}\oldShaded}{\endoldShaded\end{spacing}}
  \renewenvironment{verbatim}{\tiny\begin{spacing}{0.9}\oldverbatim}{\endoldverbatim\end{spacing}}
}

\newcommand{\scriptoutput}{
  \renewenvironment{Shaded}{\scriptsize\begin{spacing}{0.9}\oldShaded}{\endoldShaded\end{spacing}}
  \renewenvironment{verbatim}{\scriptsize\begin{spacing}{0.9}\oldverbatim}{\endoldverbatim\end{spacing}}
}

\newcommand{\footnoteoutput}{
  \renewenvironment{Shaded}{\footnotesize\begin{spacing}{0.9}\oldShaded}{\endoldShaded\end{spacing}}
  \renewenvironment{verbatim}{\footnotesize\begin{spacing}{0.9}\oldverbatim}{\endoldverbatim\end{spacing}}
}

%\newcommand{\verbatimfont}[1]{\renewcommand{\verbatim@font}{\ttfamily#1}}


%%%%%%%%%%%%%%%%
% Custom Colors
%%%%%%%%%%%%%%%%

\definecolor{redhl}{rgb}{0.98,0.29,0.28}
\definecolor{yellowhl}{rgb}{0.98,0.87,0.28}


\xdefinecolor{oiBlue}{rgb}{0.15, 0.35, 0.55}
\xdefinecolor{gray}{rgb}{0.5, 0.5, 0.5}
\xdefinecolor{darkGray}{rgb}{0.3, 0.3, 0.3}
\xdefinecolor{darkerGray}{rgb}{0.2, 0.2, 0.2}
\xdefinecolor{rubineRed}{rgb}{0.89,0,0.30}
\xdefinecolor{linkCol}{rgb}{0.11,0.49,0.95}	
\xdefinecolor{irishGreen}{rgb}{0,0.60,0}	
\xdefinecolor{darkturquoise}{rgb}{0.44, 0.58, 0.86}
\definecolor{lightGreen}{rgb}{0.533,0.765,0.42}
%\xdefinecolor{hlblue}{rgb}{0.051,0.65,1}
\xdefinecolor{hlblue}{rgb}{ 0.055, 0.639, 0.831}
\definecolor{light}{rgb}{.337,.608,.741}
\definecolor{dark}{rgb}{.337,.608,.741}

\definecolor{cpink}{rgb}{0.93, 0.23, 0.51}

%%%%%%%%%%%%%%%%
% Custom Commands
%%%%%%%%%%%%%%%%

% text colors
\newcommand{\red}[1]{\textit{\textcolor{rubineRed}{#1}}}
\newcommand{\orange}[1]{\textit{\textcolor{orange}{#1}}}
\newcommand{\pink}[1]{\textit{\textcolor{rubineRed!90!white!50}{#1}}}
\newcommand{\green}[1]{\textit{\textcolor{irishGreen}{#1}}}
\newcommand{\blue}[1]{\textit{\textcolor{darkturquoise}{#1}}}
\newcommand{\light}[1]{\textcolor{light}{\textbf{#1}}}
\newcommand{\dark}[1]{\textcolor{dark}{#1}}
\newcommand{\gray}[1]{\textcolor{gray}{#1}}


% mail
\newcommand{\mail}[1]{\href{mailto:#1}{\textit{\textcolor{linkCol}{#1}}}}

% highlighting: hl, hlGr, mathhl
\newcommand{\hl}[1]{\textit{\textcolor{hlblue}{#1}}}
\newcommand{\hlGr}[1]{\textit{\textcolor{lightGreen}{#1}}}
\newcommand{\hlRd}[1]{\textit{\textcolor{rubineRed}{#1}}}
\newcommand{\mathhl}[1]{\textcolor{hlblue}{\ensuremath{#1}}}
\newcommand{\hlr}[1]{\fcolorbox{redhl}{white}{$\displaystyle #1$}}
\newcommand{\hly}[1]{\fcolorbox{yellowhl}{white}{$\displaystyle #1$}}


\newcommand{\vvfill}{\vskip0pt plus 1filll}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\title{Lecture 10}
\providecommand{\subtitle}[1]{}
\subtitle{Forecasting and Fitting ARIMA Models}
\date{2/20/2018}

\begin{document}
\frame{\titlepage}

\hypertarget{forecasting}{%
\section{Forecasting}\label{forecasting}}

\begin{frame}[t]{%
\protect\hypertarget{forecasting-arma}{%
Forecasting ARMA}}

\begin{itemize}
\item
  Forecasts for stationary models necessarily revert to mean

  \begin{itemize}
  \item
    Remember, \(E(y_t) \ne \delta\) but rather
    \(\delta / (1 - \sum_{i=1}^p \phi_i)\).
  \item
    Differenced models revert to trend (usually a line)
  \item
    Why? AR gradually damp out, MA terms disappear
  \end{itemize}
\item
  Like any other model, accuracy decreases as we extrapolate /
  prediction interval increases
\end{itemize}

\end{frame}

\begin{frame}[t]{%
\protect\hypertarget{one-step-ahead-forecasting}{%
One step ahead forecasting}}

Take a fitted ARMA(1,1) process where we know both \(\delta\), \(\phi\),
and \(\theta\) then

\end{frame}

\begin{frame}{%
\protect\hypertarget{arima311-example}{%
ARIMA(3,1,1) example}}

\end{frame}

\hypertarget{model-fitting}{%
\section{Model Fitting}\label{model-fitting}}

\begin{frame}[t]{%
\protect\hypertarget{fitting-arima}{%
Fitting ARIMA}}

For an \(ARIMA(p,d,q)\) model

\begin{itemize}
\item
  Requires that the data be stationary after differencing \vspace{3mm}
\item
  Handling \(d\) is straight forward, just difference the original data
  \(d\) times (leaving \(n-d\) observations)
  \[ y'_t = \Delta^d \, y_t \] \vspace{3mm}
\item
  After differencing, fit an \(ARMA(p,q)\) model to \(y'_t\).
  \vspace{3mm}
\item
  To keep things simple we’ll assume
  \(w_t \overset{iid}{\sim} \mathcal{N}(0,\sigma^2_w)\)
\end{itemize}

\end{frame}

\begin{frame}{%
\protect\hypertarget{mle---stationarity-iid-normal-errors}{%
MLE - Stationarity \& iid normal errors}}

If both of these conditions are met, then the time series \(y_t\) will
also be normal.

\pause

\(~\)

In general, the vector \(\symbf{y} = (y_1, y_2, \ldots, y_t)'\) will
have a multivariate normal distribution with mean
\(\{\symbf\mu\}_i = E(y_i) = E(y_t)\) and covariance \(\symbf\Sigma\)
where \(\{\symbf{\Sigma}\}_{ij} = \gamma_{i-j}\).

\(~\)

The joint density of \(\symbf y\) is given by

\[ f_{\symbf y}(\symbf y) = \frac{1}{(2\pi)^{t/2}\,\det(\symbf\Sigma)^{1/2}} \times \exp\left( -\frac{1}{2}(\symbf y - \symbf \mu)' \, \Sigma^{-1} \, (\symbf y - \symbf \mu) \right) \]

\end{frame}

\hypertarget{ar}{%
\section{AR}\label{ar}}

\begin{frame}[t]{%
\protect\hypertarget{fitting-ar1}{%
Fitting \(AR(1)\)}}

\[ y_t = \delta + \phi \, y_{t-1} + w_t \]

We need to estimate three parameters: \(\delta\), \(\phi\), and
\(\sigma_w^2\), we know

\[ 
\begin{aligned}
E(y_t) &= \frac{\delta}{1-\phi} \\
Var(y_t) &= \frac{\sigma_w^2}{1-\phi^2} \\
\gamma_h &= \frac{\sigma_w^2}{1-\phi^2} \phi^{|h|}
\end{aligned} 
\]

Using these properties it is possible to write the distribution of
\(\symbf{y}\) as a MVN but that does not make it possible to write down
closed forms for the MLE estimate for \(\delta\), \(\phi\), and
\(\sigma_w^2\).

\end{frame}

\begin{frame}{%
\protect\hypertarget{conditional-density}{%
Conditional Density}}

We can rewrite the density as follows,

\[
\begin{aligned}
f_{\symbf y}
  &= f_{y_t,\,y_{t-1},\,\ldots,\,y_2,\,y_1} \\
  &= f_{y_t|\,y_{t-1},\,\ldots,\,y_2,\,y_1} f_{y_{t-1}|y_{t-2},\,\ldots,\,y_2,\,y_1} \cdots f_{y_2|y_1} f_{y_1} \\
  &= f_{y_t|\,y_{t-1}} f_{y_{t-1}|y_{t-2}} \cdots f_{y_2|y_1} f_{y_1} 
\end{aligned}
\]

where,

\[
\begin{aligned}
y_1 &\sim \mathcal{N}\left(\delta, \, \frac{\sigma^2_w}{1-\phi^2} \right) \\
y_{t}|y_{t-1} &\sim \mathcal{N}\left(\delta+\phi\, y_{t-1}, \, \sigma^2_w \right) \\
f_{y_{t}|y_{t-1}}(y_t) &= \frac{1}{\sqrt{2\pi \, \sigma^2_w}} \exp \left( -\frac{1}{2}\frac{(y_t -\delta+\phi\, y_{t-1})^2 }{\sigma^2_w} \right)
\end{aligned}
\]

\end{frame}

\begin{frame}[t]{%
\protect\hypertarget{log-likelihood-of-ar1}{%
Log likelihood of AR(1)}}

\scriptsize

\[
\log f_{y_{t} | y_{t-1}}(y_t) = -\frac{1}{2}\left( \log 2\pi + \log \sigma^2_w + \frac{1}{\sigma_w^2} (y_t -\delta+\phi\, y_{t-1})^2 \right)
\]

\[
\begin{aligned}
\ell(\delta, \phi, \sigma^2_w) 
  &= \log f_{\symbf{y}} = \log f_{y_1} + \sum_{i=2}^t \log f_{y_{i}|y_{i-1}} \\
  &= - \frac{1}{2} \bigg(\log 2\pi + \log \sigma_w^2 - \log (1-\phi^2) + \frac{(1-\phi^2)}{\sigma_w^2 }(y_1-\delta)^2 \bigg) \\
  & ~~~~ - \frac{1}{2} \bigg( (n-1) \log 2\pi + (n-1) \log \sigma_w^2 + \frac{1}{\sigma_w^2} \sum_{i=2}^n (y_i -\delta+\phi\, y_{i-1})^2 \bigg) \\
  &= - \frac{1}{2} \bigg( n \log 2\pi + n \log \sigma_w^2 - \log (1-\phi^2) \\
  &~~~~~~~~~~~~~~~+ \frac{1}{\sigma_w^2} \bigg( (1-\phi^2)(y_1-\delta)^2 + \sum_{i=2}^n (y_i -\delta+\phi\, y_{i-1})^2 \bigg) \bigg)
\end{aligned}
\]

\end{frame}

\begin{frame}[t]{%
\protect\hypertarget{ar1-example}{%
AR(1) Example}}

with \(\phi = -0.75\), \(\delta=0.5\), and \(\sigma_w^2=1\),

\begin{center}\includegraphics[width=\textwidth]{Lec10_files/figure-beamer/unnamed-chunk-1-1} \end{center}

\end{frame}

\begin{frame}[fragile,t]{%
\protect\hypertarget{arima}{%
Arima}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ar1_arima =}\StringTok{ }\NormalTok{forecast}\OperatorTok{::}\KeywordTok{Arima}\NormalTok{(ar1, }\DataTypeTok{order =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{)) }
\KeywordTok{summary}\NormalTok{(ar1_arima)}
\NormalTok{## Series: ar1 }
\NormalTok{## ARIMA(1,0,0) with non-zero mean }
\NormalTok{## }
\NormalTok{## Coefficients:}
\NormalTok{##          ar1    mean}
\NormalTok{##       0.6953  1.3708}
\NormalTok{## s.e.  0.0510  0.2296}
\NormalTok{## }
\NormalTok{## sigma^2 estimated as 1.011:  log likelihood=-284.23}
\NormalTok{## AIC=574.45   AICc=574.57   BIC=584.35}
\NormalTok{## }
\NormalTok{## Training set error measures:}
\NormalTok{##                       ME     RMSE       MAE       MPE     MAPE      MASE}
\NormalTok{## Training set 0.009862074 1.000539 0.7890342 -2924.705 3132.561 0.9192265}
\NormalTok{##                     ACF1}
\NormalTok{## Training set -0.01152999}
\end{Highlighting}
\end{Shaded}

\end{frame}

\begin{frame}[fragile,t]{%
\protect\hypertarget{lm}{%
lm}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d =}\StringTok{ }\KeywordTok{data_frame}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ ar1 }\OperatorTok{%>%}\StringTok{ }\KeywordTok{strip_attrs}\NormalTok{(), }\DataTypeTok{t=}\KeywordTok{seq_along}\NormalTok{(ar1))}
\NormalTok{ar1_lm =}\StringTok{ }\KeywordTok{lm}\NormalTok{(y}\OperatorTok{~}\KeywordTok{lag}\NormalTok{(y), }\DataTypeTok{data=}\NormalTok{d)}
\KeywordTok{summary}\NormalTok{(ar1_lm)}
\NormalTok{## }
\NormalTok{## Call:}
\NormalTok{## lm(formula = y ~ lag(y), data = d)}
\NormalTok{## }
\NormalTok{## Residuals:}
\NormalTok{##      Min       1Q   Median       3Q      Max }
\NormalTok{## -3.06604 -0.63196  0.01817  0.70052  2.60532 }
\NormalTok{## }
\NormalTok{## Coefficients:}
\NormalTok{##             Estimate Std. Error t value Pr(>|t|)    }
\NormalTok{## (Intercept)  0.44017    0.09985   4.408 1.71e-05 ***}
\NormalTok{## lag(y)       0.69144    0.05125  13.491  < 2e-16 ***}
\NormalTok{## ---}
\NormalTok{## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1}
\NormalTok{## }
\NormalTok{## Residual standard error: 1.003 on 197 degrees of freedom}
\NormalTok{##   (1 observation deleted due to missingness)}
\NormalTok{## Multiple R-squared:  0.4802, Adjusted R-squared:  0.4776 }
\NormalTok{## F-statistic:   182 on 1 and 197 DF,  p-value: < 2.2e-16}
\end{Highlighting}
\end{Shaded}

\end{frame}

\begin{frame}[fragile,t]{%
\protect\hypertarget{bayesian-ar1-model}{%
Bayesian AR(1) Model}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ar1_model =}\StringTok{ "model\{}
\StringTok{# likelihood}
\StringTok{  y[1] ~ dnorm(delta/(1-phi), (sigma2_w/(1-phi^2))^-1)}
\StringTok{  y_hat[1] ~ dnorm(delta/(1-phi), (sigma2_w/(1-phi^2))^-1)}

\StringTok{  for (t in 2:length(y)) \{}
\StringTok{    y[t] ~ dnorm(delta + phi*y[t-1], 1/sigma2_w)}
\StringTok{    y_hat[t] ~ dnorm(delta + phi*y[t-1], 1/sigma2_w)}
\StringTok{  \}}
\StringTok{  }
\StringTok{  mu = delta/(1-phi)}

\StringTok{# priors}
\StringTok{  delta ~ dnorm(0,1/1000)}
\StringTok{  phi ~ dnorm(0,1)}
\StringTok{  tau ~ dgamma(0.001,0.001)}
\StringTok{  sigma2_w <- 1/tau}
\StringTok{\}"}
\end{Highlighting}
\end{Shaded}

\end{frame}

\begin{frame}{%
\protect\hypertarget{chains}{%
Chains}}

\begin{center}\includegraphics[width=\textwidth]{Lec10_files/figure-beamer/unnamed-chunk-7-1} \end{center}

\end{frame}

\begin{frame}{%
\protect\hypertarget{posteriors}{%
Posteriors}}

\begin{center}\includegraphics[width=\textwidth]{Lec10_files/figure-beamer/unnamed-chunk-8-1} \end{center}

\end{frame}

\begin{frame}{%
\protect\hypertarget{predictions}{%
Predictions}}

\begin{center}\includegraphics[width=\textwidth]{Lec10_files/figure-beamer/unnamed-chunk-9-1} \end{center}

\end{frame}

\begin{frame}{%
\protect\hypertarget{faceted}{%
Faceted}}

\begin{center}\includegraphics[width=\textwidth]{Lec10_files/figure-beamer/unnamed-chunk-10-1} \end{center}

\end{frame}

\begin{frame}[t]{%
\protect\hypertarget{fitting-arp---lagged-regression}{%
Fitting AR(p) - Lagged Regression}}

We can rewrite the density as follows, \[
\begin{aligned}
f(\symbf y)
  &= f(y_t, \,y_{t-1}, \,\ldots, \,y_{2}, \,y_{1}) \\
  &= f(y_{n}|y_{n-1},\ldots,y_{n-p}) \cdots  f(y_{p+1}|y_p,\ldots,y_1)  f(y_p, \,\ldots, y_1)
\end{aligned}
\]

\pause

Regressing \(y_t\) on \(y_{t-p}, \ldots, y_{t-1}\) gets us an
approximate solution, but it ignores the
\(f(y_1, \, y_2, \,\ldots, y_p)\) part of the likelihood.

How much does this matter (vs.~using the full likelihood)?

\begin{itemize}
\item
  If \(p\) is near to \(n\) then probably a lot
\item
  If \(p << n\) then probably not much
\end{itemize}

\end{frame}

\begin{frame}[t]{%
\protect\hypertarget{fitting-arp---method-of-moments}{%
Fitting AR(p) - Method of Moments}}

Recall for an AR(p) process,

\[
\begin{aligned}
\gamma(0) &= \sigma^2_w + \phi_1 \gamma(1) + \phi_2 \gamma(2) + \ldots + \phi_p \gamma(p) \\
\gamma(h) &= \phi_1 \gamma(h-1) + \phi_2 \gamma(h-2) + \ldots \phi_p \gamma(h-p)
\end{aligned}
\] We can rewrite the first equation in terms of \(\sigma^2_w\), \[
\sigma^2_w =  \gamma(0) - \phi_1 \gamma(1) - \phi_2 \gamma(2) - \ldots - \phi_p \gamma(p)
\] these are called the Yule-Walker equations.

\end{frame}

\begin{frame}[t]{%
\protect\hypertarget{yule-walker}{%
Yule-Walker}}

These equations can be rewritten into matrix notation as follows

\[
\underset{p \times p}{\symbf\Gamma_p} \underset{p \times 1}{\symbf\phi} = \underset{p \times 1}{\symbf\gamma_p}
\qquad\qquad
\underset{1 \times 1}{\sigma^2_w} = \underset{1 \times 1}{\gamma(0)} - \underset{1 \times p}{\symbf{\phi'}}\underset{p \times 1}{\symbf{\gamma_p}}
\] where \[ 
\begin{aligned}
\underset{p \times p}{\symbf{\Gamma_p}} &= \{\gamma(j-k)\}_{j,k} \\
\underset{p \times 1}{\symbf\phi} &= (\phi_1, \phi_2, \ldots, \phi_p)' \\
\underset{p \times 1}{\symbf\gamma_p} &= (\gamma(1), \gamma(2), \ldots, \gamma(p))'
\end{aligned}
\]

\pause

If we estimate the covariance structure from the data we obtain
\(\hat{\symbf\gamma_p}\) which can plug in and solve for
\(\symbf{\phi}\) and \(\sigma^2_w\),

\[
\hat{\symbf\phi} =\hat{\symbf{\Gamma}_p}^{-1}\hat{\symbf{\gamma}_p}
\qquad\qquad
\sigma^2_w = \gamma(0) - \hat{\symbf{\gamma}_p}' \hat{\symbf{\Gamma}_p^{-1}} \hat{\symbf{\gamma}_p}
\]

\end{frame}

\hypertarget{arma}{%
\section{ARMA}\label{arma}}

\begin{frame}[t]{%
\protect\hypertarget{fitting-arma22}{%
Fitting \(ARMA(2,2)\)}}

\[ y_t = \delta + \phi_1 \, y_{t-1} + \phi_2 \, y_{t-2} + \theta_1 w_{t-1} + \theta_2 w_{t-2} + w_t \]

Need to estimate six parameters: \(\delta\), \(\phi_1\), \(\phi_2\),
\(\theta_1\), \(\theta_2\) and \(\sigma_w^2\).

\pause

\(~\)

We could figure out \(E(y_t)\), \(Var(y_t)\), and \(Cov(y_t, y_{t+h})\),
but the last two are going to be pretty nasty and the full MVN likehood
is similarly going to be unpleasant to work with.

\pause

\(~\)

Like the AR(1) and AR(p) processes we want to use conditioning to
simplify things. \tinyoutput \[
\begin{aligned}
y_t | \delta, &y_{t-1}, y_{t-2}, w_{t-1}, w_{t-2} \\ 
&\sim \mathcal{N}(\delta + \phi_1 \, y_{t-1} + \phi_2 \, y_{t-2} + \theta_1 w_{t-1} + \theta_2 w_{t-2},~\sigma_w^2) 
\end{aligned}
\]

\end{frame}

\begin{frame}{%
\protect\hypertarget{arma22-example}{%
ARMA(2,2) Example}}

with \(\phi = (1.3,-0.5)\), \(\theta = (0.5,0.2)\), \(\delta=0\), and
\(\sigma_w^2=1\) using the same models

\begin{center}\includegraphics[width=\textwidth]{Lec10_files/figure-beamer/unnamed-chunk-11-1} \end{center}

\end{frame}

\begin{frame}[fragile]{%
\protect\hypertarget{arima-1}{%
ARIMA}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{forecast}\OperatorTok{::}\KeywordTok{Arima}\NormalTok{(y, }\DataTypeTok{order =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{2}\NormalTok{), }\DataTypeTok{include.mean =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{summary}\NormalTok{()}
\NormalTok{## Series: y }
\NormalTok{## ARIMA(2,0,2) with zero mean }
\NormalTok{## }
\NormalTok{## Coefficients:}
\NormalTok{##          ar1      ar2     ma1     ma2}
\NormalTok{##       1.3171  -0.5142  0.4332  0.1651}
\NormalTok{## s.e.  0.0881   0.0784  0.0955  0.0865}
\NormalTok{## }
\NormalTok{## sigma^2 estimated as 0.9517:  log likelihood=-696.8}
\NormalTok{## AIC=1403.61   AICc=1403.73   BIC=1424.68}
\NormalTok{## }
\NormalTok{## Training set error measures:}
\NormalTok{##                      ME      RMSE       MAE      MPE     MAPE      MASE}
\NormalTok{## Training set 0.05247101 0.9716288 0.7755171 19.94389 95.20779 0.6500169}
\NormalTok{##                     ACF1}
\NormalTok{## Training set -0.01079679}
\end{Highlighting}
\end{Shaded}

\end{frame}

\begin{frame}[fragile]{%
\protect\hypertarget{ar-only-lm}{%
AR only lm}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{lm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\KeywordTok{lag}\NormalTok{(y,}\DecValTok{1}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{lag}\NormalTok{(y,}\DecValTok{2}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{summary}\NormalTok{()}
\NormalTok{## }
\NormalTok{## Call:}
\NormalTok{## lm(formula = y ~ lag(y, 1) + lag(y, 2))}
\NormalTok{## }
\NormalTok{## Residuals:}
\NormalTok{##     Min      1Q  Median      3Q     Max }
\NormalTok{## -2.7711 -0.6422  0.0119  0.6696  3.2150 }
\NormalTok{## }
\NormalTok{## Coefficients:}
\NormalTok{##             Estimate Std. Error t value Pr(>|t|)    }
\NormalTok{## (Intercept)  0.06476    0.04502   1.438    0.151    }
\NormalTok{## lag(y, 1)    1.55990    0.03063  50.921   <2e-16 ***}
\NormalTok{## lag(y, 2)   -0.72697    0.03044 -23.879   <2e-16 ***}
\NormalTok{## ---}
\NormalTok{## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1}
\NormalTok{## }
\NormalTok{## Residual standard error: 0.9969 on 495 degrees of freedom}
\NormalTok{##   (2 observations deleted due to missingness)}
\NormalTok{## Multiple R-squared:  0.9147, Adjusted R-squared:  0.9144 }
\NormalTok{## F-statistic:  2654 on 2 and 495 DF,  p-value: < 2.2e-16}
\end{Highlighting}
\end{Shaded}

\end{frame}

\begin{frame}[t]{%
\protect\hypertarget{hannan-rissanen-algorithm}{%
Hannan-Rissanen Algorithm}}

\begin{enumerate}
[1.]
\tightlist
\item
  Estimate a high order AR (remember AR \(\Leftrightarrow\) MA when
  stationary + invertible)
\end{enumerate}

\vspace{5mm}

\begin{enumerate}
[1.]
\setcounter{enumi}{1}
\tightlist
\item
  Use AR to estimate values for unobserved \(w_t\)
\end{enumerate}

\vspace{5mm}

\begin{enumerate}
[1.]
\setcounter{enumi}{2}
\tightlist
\item
  Regress \(y_t\) onto
  \(y_{t-1}, \ldots, y_{t-p}, \hat{w}_{t-1}, \ldots \hat{w}_{t-q}\)
\end{enumerate}

\vspace{5mm}

\begin{enumerate}
[1.]
\setcounter{enumi}{3}
\tightlist
\item
  Update \(\hat{w}_{t-1}, \ldots \hat{w}_{t-q}\) based on current model,
  refit and then repeat until convergence
\end{enumerate}

\end{frame}

\begin{frame}[fragile,t]{%
\protect\hypertarget{hannan-rissanen---step-1-2}{%
Hannan-Rissanen - Step 1 \& 2}}

\scriptoutput

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ar =}\StringTok{ }\KeywordTok{ar.mle}\NormalTok{(y, }\DataTypeTok{order.max =} \DecValTok{10}\NormalTok{)}
\NormalTok{ar}
\NormalTok{## }
\NormalTok{## Call:}
\NormalTok{## ar.mle(x = y, order.max = 10)}
\NormalTok{## }
\NormalTok{## Coefficients:}
\NormalTok{##       1        2        3        4        5        6        7        8  }
\NormalTok{##  1.7554  -1.1376   0.2222   0.0881  -0.0081  -0.2006   0.3130  -0.2674  }
\NormalTok{##       9  }
\NormalTok{##  0.0949  }
\NormalTok{## }
\NormalTok{## Order selected 9  sigma^2 estimated as  0.916}
\end{Highlighting}
\end{Shaded}

\end{frame}

\begin{frame}[fragile]{%
\protect\hypertarget{residuals}{%
Residuals}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{forecast}\OperatorTok{::}\KeywordTok{ggtsdisplay}\NormalTok{(ar}\OperatorTok{$}\NormalTok{resid)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{Lec10_files/figure-beamer/unnamed-chunk-15-1} \end{center}

\end{frame}

\begin{frame}[fragile]{%
\protect\hypertarget{hannan-rissanen---step-3}{%
Hannan-Rissanen - Step 3}}

\scriptoutput

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d =}\StringTok{ }\KeywordTok{data_frame}\NormalTok{(}
  \DataTypeTok{y =}\NormalTok{ y }\OperatorTok{%>%}\StringTok{ }\KeywordTok{strip_attrs}\NormalTok{(), }
  \DataTypeTok{w_hat1 =}\NormalTok{ ar}\OperatorTok{$}\NormalTok{resid }\OperatorTok{%>%}\StringTok{ }\KeywordTok{strip_attrs}\NormalTok{()}
\NormalTok{)}

\NormalTok{(}\DataTypeTok{lm1 =} \KeywordTok{lm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\KeywordTok{lag}\NormalTok{(y,}\DecValTok{1}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{lag}\NormalTok{(y,}\DecValTok{2}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{lag}\NormalTok{(w_hat1,}\DecValTok{1}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{lag}\NormalTok{(w_hat1,}\DecValTok{2}\NormalTok{), }\DataTypeTok{data=}\NormalTok{d)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summary}\NormalTok{()}
\NormalTok{## }
\NormalTok{## Call:}
\NormalTok{## lm(formula = y ~ lag(y, 1) + lag(y, 2) + lag(w_hat1, 1) + lag(w_hat1, }
\NormalTok{##     2), data = d)}
\NormalTok{## }
\NormalTok{## Residuals:}
\NormalTok{##      Min       1Q   Median       3Q      Max }
\NormalTok{## -2.49762 -0.67125  0.02098  0.60126  2.91051 }
\NormalTok{## }
\NormalTok{## Coefficients:}
\NormalTok{##                Estimate Std. Error t value Pr(>|t|)    }
\NormalTok{## (Intercept)     0.09387    0.04428   2.120   0.0345 *  }
\NormalTok{## lag(y, 1)       1.32970    0.06119  21.732  < 2e-16 ***}
\NormalTok{## lag(y, 2)      -0.52939    0.05275 -10.035  < 2e-16 ***}
\NormalTok{## lag(w_hat1, 1)  0.41219    0.07617   5.412 9.84e-08 ***}
\NormalTok{## lag(w_hat1, 2)  0.13377    0.07335   1.824   0.0688 .  }
\NormalTok{## ---}
\NormalTok{## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1}
\NormalTok{## }
\NormalTok{## Residual standard error: 0.9638 on 484 degrees of freedom}
\NormalTok{##   (11 observations deleted due to missingness)}
\NormalTok{## Multiple R-squared:  0.918,  Adjusted R-squared:  0.9174 }
\NormalTok{## F-statistic:  1355 on 4 and 484 DF,  p-value: < 2.2e-16}
\end{Highlighting}
\end{Shaded}

\end{frame}

\begin{frame}[fragile]{%
\protect\hypertarget{hannan-rissanen---step-4.1}{%
Hannan-Rissanen - Step 4.1}}

\scriptoutput

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d =}\StringTok{ }\NormalTok{modelr}\OperatorTok{::}\KeywordTok{add_residuals}\NormalTok{(d,lm1,}\StringTok{"w_hat2"}\NormalTok{)}

\NormalTok{(}\DataTypeTok{lm2 =} \KeywordTok{lm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\KeywordTok{lag}\NormalTok{(y,}\DecValTok{1}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{lag}\NormalTok{(y,}\DecValTok{2}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{lag}\NormalTok{(w_hat2,}\DecValTok{1}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{lag}\NormalTok{(w_hat2,}\DecValTok{2}\NormalTok{), }\DataTypeTok{data=}\NormalTok{d)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summary}\NormalTok{()}
\NormalTok{## }
\NormalTok{## Call:}
\NormalTok{## lm(formula = y ~ lag(y, 1) + lag(y, 2) + lag(w_hat2, 1) + lag(w_hat2, }
\NormalTok{##     2), data = d)}
\NormalTok{## }
\NormalTok{## Residuals:}
\NormalTok{##      Min       1Q   Median       3Q      Max }
\NormalTok{## -2.45706 -0.66872  0.04817  0.61811  2.83199 }
\NormalTok{## }
\NormalTok{## Coefficients:}
\NormalTok{##                Estimate Std. Error t value Pr(>|t|)    }
\NormalTok{## (Intercept)     0.09550    0.04460   2.141   0.0328 *  }
\NormalTok{## lag(y, 1)       1.31434    0.06434  20.429  < 2e-16 ***}
\NormalTok{## lag(y, 2)      -0.51885    0.05473  -9.481  < 2e-16 ***}
\NormalTok{## lag(w_hat2, 1)  0.42052    0.07902   5.321 1.58e-07 ***}
\NormalTok{## lag(w_hat2, 2)  0.14499    0.07560   1.918   0.0557 .  }
\NormalTok{## ---}
\NormalTok{## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1}
\NormalTok{## }
\NormalTok{## Residual standard error: 0.9663 on 482 degrees of freedom}
\NormalTok{##   (13 observations deleted due to missingness)}
\NormalTok{## Multiple R-squared:  0.9168, Adjusted R-squared:  0.9161 }
\NormalTok{## F-statistic:  1328 on 4 and 482 DF,  p-value: < 2.2e-16}
\end{Highlighting}
\end{Shaded}

\end{frame}

\begin{frame}[fragile]{%
\protect\hypertarget{hannan-rissanen---step-4.2}{%
Hannan-Rissanen - Step 4.2}}

\scriptoutput

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d =}\StringTok{ }\NormalTok{modelr}\OperatorTok{::}\KeywordTok{add_residuals}\NormalTok{(d,lm2,}\StringTok{"w_hat3"}\NormalTok{)}

\NormalTok{(}\DataTypeTok{lm3 =} \KeywordTok{lm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\KeywordTok{lag}\NormalTok{(y,}\DecValTok{1}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{lag}\NormalTok{(y,}\DecValTok{2}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{lag}\NormalTok{(w_hat3,}\DecValTok{1}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{lag}\NormalTok{(w_hat3,}\DecValTok{2}\NormalTok{), }\DataTypeTok{data=}\NormalTok{d)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summary}\NormalTok{()}
\NormalTok{## }
\NormalTok{## Call:}
\NormalTok{## lm(formula = y ~ lag(y, 1) + lag(y, 2) + lag(w_hat3, 1) + lag(w_hat3, }
\NormalTok{##     2), data = d)}
\NormalTok{## }
\NormalTok{## Residuals:}
\NormalTok{##      Min       1Q   Median       3Q      Max }
\NormalTok{## -2.48875 -0.67021  0.00531  0.61885  2.89492 }
\NormalTok{## }
\NormalTok{## Coefficients:}
\NormalTok{##                Estimate Std. Error t value Pr(>|t|)    }
\NormalTok{## (Intercept)     0.09424    0.04484   2.102   0.0361 *  }
\NormalTok{## lag(y, 1)       1.31907    0.06520  20.232  < 2e-16 ***}
\NormalTok{## lag(y, 2)      -0.52227    0.05557  -9.398  < 2e-16 ***}
\NormalTok{## lag(w_hat3, 1)  0.41250    0.07983   5.167  3.5e-07 ***}
\NormalTok{## lag(w_hat3, 2)  0.13981    0.07620   1.835   0.0671 .  }
\NormalTok{## ---}
\NormalTok{## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1}
\NormalTok{## }
\NormalTok{## Residual standard error: 0.9692 on 480 degrees of freedom}
\NormalTok{##   (15 observations deleted due to missingness)}
\NormalTok{## Multiple R-squared:  0.9166, Adjusted R-squared:  0.9159 }
\NormalTok{## F-statistic:  1320 on 4 and 480 DF,  p-value: < 2.2e-16}
\end{Highlighting}
\end{Shaded}

\end{frame}

\begin{frame}[fragile]{%
\protect\hypertarget{hannan-rissanen---step-4.3}{%
Hannan-Rissanen - Step 4.3}}

\scriptoutput

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d =}\StringTok{ }\NormalTok{modelr}\OperatorTok{::}\KeywordTok{add_residuals}\NormalTok{(d,lm3,}\StringTok{"w_hat4"}\NormalTok{)}

\NormalTok{(}\DataTypeTok{lm4 =} \KeywordTok{lm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\KeywordTok{lag}\NormalTok{(y,}\DecValTok{1}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{lag}\NormalTok{(y,}\DecValTok{2}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{lag}\NormalTok{(w_hat4,}\DecValTok{1}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{lag}\NormalTok{(w_hat4,}\DecValTok{2}\NormalTok{), }\DataTypeTok{data=}\NormalTok{d)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summary}\NormalTok{()}
\NormalTok{## }
\NormalTok{## Call:}
\NormalTok{## lm(formula = y ~ lag(y, 1) + lag(y, 2) + lag(w_hat4, 1) + lag(w_hat4, }
\NormalTok{##     2), data = d)}
\NormalTok{## }
\NormalTok{## Residuals:}
\NormalTok{##      Min       1Q   Median       3Q      Max }
\NormalTok{## -2.47852 -0.64596  0.01075  0.61344  2.90021 }
\NormalTok{## }
\NormalTok{## Coefficients:}
\NormalTok{##                Estimate Std. Error t value Pr(>|t|)    }
\NormalTok{## (Intercept)     0.09868    0.04474   2.205   0.0279 *  }
\NormalTok{## lag(y, 1)       1.33693    0.06573  20.339  < 2e-16 ***}
\NormalTok{## lag(y, 2)      -0.53737    0.05607  -9.584  < 2e-16 ***}
\NormalTok{## lag(w_hat4, 1)  0.38853    0.08042   4.831 1.83e-06 ***}
\NormalTok{## lag(w_hat4, 2)  0.12337    0.07625   1.618   0.1063    }
\NormalTok{## ---}
\NormalTok{## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1}
\NormalTok{## }
\NormalTok{## Residual standard error: 0.9661 on 478 degrees of freedom}
\NormalTok{##   (17 observations deleted due to missingness)}
\NormalTok{## Multiple R-squared:  0.9174, Adjusted R-squared:  0.9167 }
\NormalTok{## F-statistic:  1327 on 4 and 478 DF,  p-value: < 2.2e-16}
\end{Highlighting}
\end{Shaded}

\end{frame}

\begin{frame}[fragile]{%
\protect\hypertarget{hannan-rissanen---step-4.4}{%
Hannan-Rissanen - Step 4.4}}

\scriptoutput

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d =}\StringTok{ }\NormalTok{modelr}\OperatorTok{::}\KeywordTok{add_residuals}\NormalTok{(d,lm4,}\StringTok{"w_hat5"}\NormalTok{)}

\NormalTok{(}\DataTypeTok{lm5 =} \KeywordTok{lm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\KeywordTok{lag}\NormalTok{(y,}\DecValTok{1}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{lag}\NormalTok{(y,}\DecValTok{2}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{lag}\NormalTok{(w_hat5,}\DecValTok{1}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{lag}\NormalTok{(w_hat5,}\DecValTok{2}\NormalTok{), }\DataTypeTok{data=}\NormalTok{d)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summary}\NormalTok{()}
\NormalTok{## }
\NormalTok{## Call:}
\NormalTok{## lm(formula = y ~ lag(y, 1) + lag(y, 2) + lag(w_hat5, 1) + lag(w_hat5, }
\NormalTok{##     2), data = d)}
\NormalTok{## }
\NormalTok{## Residuals:}
\NormalTok{##     Min      1Q  Median      3Q     Max }
\NormalTok{## -2.4846 -0.6551  0.0259  0.6065  2.9046 }
\NormalTok{## }
\NormalTok{## Coefficients:}
\NormalTok{##                Estimate Std. Error t value Pr(>|t|)    }
\NormalTok{## (Intercept)     0.10066    0.04500   2.237   0.0258 *  }
\NormalTok{## lag(y, 1)       1.34414    0.06521  20.612  < 2e-16 ***}
\NormalTok{## lag(y, 2)      -0.54288    0.05566  -9.754  < 2e-16 ***}
\NormalTok{## lag(w_hat5, 1)  0.38065    0.07984   4.768 2.48e-06 ***}
\NormalTok{## lag(w_hat5, 2)  0.11431    0.07581   1.508   0.1323    }
\NormalTok{## ---}
\NormalTok{## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1}
\NormalTok{## }
\NormalTok{## Residual standard error: 0.9675 on 476 degrees of freedom}
\NormalTok{##   (19 observations deleted due to missingness)}
\NormalTok{## Multiple R-squared:  0.9168, Adjusted R-squared:  0.9161 }
\NormalTok{## F-statistic:  1312 on 4 and 476 DF,  p-value: < 2.2e-16}
\end{Highlighting}
\end{Shaded}

\end{frame}

\begin{frame}[fragile]{%
\protect\hypertarget{rmses}{%
RMSEs}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{modelr}\OperatorTok{::}\KeywordTok{rmse}\NormalTok{(lm1, }\DataTypeTok{data =}\NormalTok{ d)}
\NormalTok{## [1] 0.9588745}

\NormalTok{modelr}\OperatorTok{::}\KeywordTok{rmse}\NormalTok{(lm2, }\DataTypeTok{data =}\NormalTok{ d)}
\NormalTok{## [1] 0.9613757}

\NormalTok{modelr}\OperatorTok{::}\KeywordTok{rmse}\NormalTok{(lm3, }\DataTypeTok{data =}\NormalTok{ d)}
\NormalTok{## [1] 0.9641811}

\NormalTok{modelr}\OperatorTok{::}\KeywordTok{rmse}\NormalTok{(lm4, }\DataTypeTok{data =}\NormalTok{ d)}
\NormalTok{## [1] 0.9611267}

\NormalTok{modelr}\OperatorTok{::}\KeywordTok{rmse}\NormalTok{(lm5, }\DataTypeTok{data =}\NormalTok{ d)}
\NormalTok{## [1] 0.9624088}
\end{Highlighting}
\end{Shaded}

\end{frame}

\begin{frame}[fragile]{%
\protect\hypertarget{bayesian-model}{%
Bayesian Model}}

\tinyoutput

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{arma22_model =}\StringTok{ "model\{}
\StringTok{# Likelihood}
\StringTok{  for (t in 1:length(y)) \{}
\StringTok{    y[t] ~ dnorm(mu[t], 1/sigma2_e)}
\StringTok{  \}                                   }

\StringTok{  mu[1] = phi[1] * y_0  + phi[2] * y_n1 + w[1] + theta[1]*w_0  + theta[2]*w_n1}
\StringTok{  mu[2] = phi[1] * y[1] + phi[2] * y_0  + w[2] + theta[1]*w[1] + theta[2]*w_0   }
\StringTok{  for (t in 3:length(y)) \{ }
\StringTok{    mu[t] = phi[1] * y[t-1] + phi[2] * y[t-2] + w[t] + theta[1] * w[t-1] + theta[2] * w[t-2]}
\StringTok{  \}}
\StringTok{  }
\StringTok{# Priors}
\StringTok{  for(t in 1:length(y))\{}
\StringTok{    w[t] ~ dnorm(0,1/sigma2_w)}
\StringTok{  \}}

\StringTok{  sigma2_w = 1/tau_w; tau_w ~ dgamma(0.001, 0.001) }
\StringTok{  sigma2_e = 1/tau_e; tau_e ~ dgamma(0.001, 0.001) }
\StringTok{  for(i in 1:2) \{}
\StringTok{    phi[i] ~ dnorm(0,1)}
\StringTok{    theta[i] ~ dnorm(0,1)}
\StringTok{  \}}

\StringTok{# Latent errors and series values}
\StringTok{  w_0  ~ dt(0,tau_w,2)}
\StringTok{  w_n1 ~ dt(0,tau_w,2)}
\StringTok{  y_0  ~ dnorm(0,1/1000)}
\StringTok{  y_n1 ~ dnorm(0,1/1000)}
\StringTok{\}"}
\end{Highlighting}
\end{Shaded}

\end{frame}

\begin{frame}{%
\protect\hypertarget{bayesian-fit}{%
Bayesian Fit}}

\begin{center}\includegraphics[width=\textwidth]{Lec10_files/figure-beamer/unnamed-chunk-24-1} \end{center}

\end{frame}

\begin{frame}{%
\protect\hypertarget{posteriors-1}{%
Posteriors}}

\begin{center}\includegraphics[width=\textwidth]{Lec10_files/figure-beamer/unnamed-chunk-25-1} \end{center}

\end{frame}

\end{document}
